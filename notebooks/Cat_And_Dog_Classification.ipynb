{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Problem Statement: **Cat V/s Dog Classification**","metadata":{}},{"cell_type":"markdown","source":"# Description:\n\n**Cat vs dog classification is a popular computer vision task that involves training a model to distinguish between images of cats and dogs. This task can be challenging, as there is a lot of variation in the appearance of cats and dogs, even within the same breed. However, with the use of deep learning techniques, it is possible to achieve high accuracy in cat vs dog classification.**\n\n**One of the most common approaches to cat vs dog classification is to use a convolutional neural network (CNN). CNNs are a type of deep learning model that are specifically designed for image classification tasks. They work by learning to identify patterns in images, and can be trained on large datasets of labeled images.**\n\n**Another approach to cat vs dog classification is to use a transfer learning model. Transfer learning is a technique where a pre-trained model is used as a starting point for training a new model. This can be helpful for cat vs dog classification, as there are many pre-trained CNN models that have been trained on large datasets of images.**\n\n**The accuracy of cat vs dog classification models can vary depending on the dataset used to train the model, the architecture of the model, and the optimization techniques used. However, it is possible to achieve accuracies of over 95% with well-trained models.**","metadata":{}},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport os.path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nimport os, cv2, re, random\nimport numpy as np\nimport pandas as pd\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing.image import img_to_array, load_img\nfrom keras import layers, models, optimizers\nfrom keras import backend as K\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-06-22T21:25:30.990690Z","iopub.execute_input":"2023-06-22T21:25:30.991052Z","iopub.status.idle":"2023-06-22T21:25:31.164643Z","shell.execute_reply.started":"2023-06-22T21:25:30.991019Z","shell.execute_reply":"2023-06-22T21:25:31.163625Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"image_dir = Path('/kaggle/input/cat-and-dog-images-dataset/Dog and Cat .png')","metadata":{"execution":{"iopub.status.busy":"2023-06-22T21:24:42.001711Z","iopub.execute_input":"2023-06-22T21:24:42.002075Z","iopub.status.idle":"2023-06-22T21:24:42.006686Z","shell.execute_reply.started":"2023-06-22T21:24:42.002043Z","shell.execute_reply":"2023-06-22T21:24:42.005569Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Create File DataFrame","metadata":{}},{"cell_type":"code","source":"filepaths = list(image_dir.glob(r'**/*.png'))\nlabels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n\nfilepaths = pd.Series(filepaths, name='Filepath').astype(str)\nlabels = pd.Series(labels, name='Label')\n\nimage_df = pd.concat([filepaths, labels], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T21:25:44.527698Z","iopub.execute_input":"2023-06-22T21:25:44.528052Z","iopub.status.idle":"2023-06-22T21:25:44.858140Z","shell.execute_reply.started":"2023-06-22T21:25:44.528020Z","shell.execute_reply":"2023-06-22T21:25:44.857301Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"image_df","metadata":{"execution":{"iopub.status.busy":"2023-06-22T21:25:48.515267Z","iopub.execute_input":"2023-06-22T21:25:48.515823Z","iopub.status.idle":"2023-06-22T21:25:48.542538Z","shell.execute_reply.started":"2023-06-22T21:25:48.515773Z","shell.execute_reply":"2023-06-22T21:25:48.541273Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                              Filepath Label\n0    /kaggle/input/cat-and-dog-images-dataset/Dog a...   Dog\n1    /kaggle/input/cat-and-dog-images-dataset/Dog a...   Dog\n2    /kaggle/input/cat-and-dog-images-dataset/Dog a...   Dog\n3    /kaggle/input/cat-and-dog-images-dataset/Dog a...   Dog\n4    /kaggle/input/cat-and-dog-images-dataset/Dog a...   Dog\n..                                                 ...   ...\n994  /kaggle/input/cat-and-dog-images-dataset/Dog a...   Cat\n995  /kaggle/input/cat-and-dog-images-dataset/Dog a...   Cat\n996  /kaggle/input/cat-and-dog-images-dataset/Dog a...   Cat\n997  /kaggle/input/cat-and-dog-images-dataset/Dog a...   Cat\n998  /kaggle/input/cat-and-dog-images-dataset/Dog a...   Cat\n\n[999 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Filepath</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/cat-and-dog-images-dataset/Dog a...</td>\n      <td>Dog</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/cat-and-dog-images-dataset/Dog a...</td>\n      <td>Dog</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/cat-and-dog-images-dataset/Dog a...</td>\n      <td>Dog</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/cat-and-dog-images-dataset/Dog a...</td>\n      <td>Dog</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/cat-and-dog-images-dataset/Dog a...</td>\n      <td>Dog</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>994</th>\n      <td>/kaggle/input/cat-and-dog-images-dataset/Dog a...</td>\n      <td>Cat</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>/kaggle/input/cat-and-dog-images-dataset/Dog a...</td>\n      <td>Cat</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>/kaggle/input/cat-and-dog-images-dataset/Dog a...</td>\n      <td>Cat</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>/kaggle/input/cat-and-dog-images-dataset/Dog a...</td>\n      <td>Cat</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>/kaggle/input/cat-and-dog-images-dataset/Dog a...</td>\n      <td>Cat</td>\n    </tr>\n  </tbody>\n</table>\n<p>999 rows Ã— 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_df, test_df = train_test_split(image_df, train_size=0.7, shuffle=True, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T21:26:27.889089Z","iopub.execute_input":"2023-06-22T21:26:27.889483Z","iopub.status.idle":"2023-06-22T21:26:27.897273Z","shell.execute_reply.started":"2023-06-22T21:26:27.889444Z","shell.execute_reply":"2023-06-22T21:26:27.896255Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Load Image Data","metadata":{}},{"cell_type":"code","source":"train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    rescale=1./255,\n    horizontal_flip=True,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    validation_split=0.2\n)\n\nval_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    rescale=1./255,\n    horizontal_flip=True,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    validation_split=0.2\n)\n\n\ntest_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    rescale=1./255\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T21:27:46.369164Z","iopub.execute_input":"2023-06-22T21:27:46.369531Z","iopub.status.idle":"2023-06-22T21:27:46.376973Z","shell.execute_reply.started":"2023-06-22T21:27:46.369485Z","shell.execute_reply":"2023-06-22T21:27:46.375731Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_images = train_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='binary',\n    batch_size=32,\n    shuffle=True,\n    seed=42,\n    subset='training'\n)\n\nval_images = val_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='binary',\n    batch_size=32,\n    shuffle=True,\n    seed=42,\n    subset='validation'\n)\n\ntest_images = test_generator.flow_from_dataframe(\n    dataframe=test_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='binary',\n    batch_size=32,\n    shuffle=False\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T21:28:13.801044Z","iopub.execute_input":"2023-06-22T21:28:13.801392Z","iopub.status.idle":"2023-06-22T21:28:14.306255Z","shell.execute_reply.started":"2023-06-22T21:28:13.801359Z","shell.execute_reply":"2023-06-22T21:28:14.305256Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Found 560 validated image filenames belonging to 2 classes.\nFound 139 validated image filenames belonging to 2 classes.\nFound 300 validated image filenames belonging to 2 classes.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"inputs = tf.keras.Input(shape=(224, 224, 3))\nx = tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu')(inputs)\nx = tf.keras.layers.MaxPool2D()(x)\nx = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(x)\nx = tf.keras.layers.MaxPool2D()(x)\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\nx = tf.keras.layers.Dense(128, activation='relu')(x)\nx = tf.keras.layers.Dense(128, activation='relu')(x)\noutputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory = model.fit(\n    train_images,\n    validation_data=val_images,\n    epochs=100,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=5,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            patience=3\n        )\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T21:29:26.047659Z","iopub.execute_input":"2023-06-22T21:29:26.048017Z","iopub.status.idle":"2023-06-22T21:34:06.402231Z","shell.execute_reply.started":"2023-06-22T21:29:26.047986Z","shell.execute_reply":"2023-06-22T21:34:06.401363Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Epoch 1/100\n18/18 [==============================] - 19s 897ms/step - loss: 0.6926 - accuracy: 0.5134 - val_loss: 0.6898 - val_accuracy: 0.4964\nEpoch 2/100\n18/18 [==============================] - 11s 626ms/step - loss: 0.6859 - accuracy: 0.5710 - val_loss: 0.6893 - val_accuracy: 0.5540\nEpoch 3/100\n18/18 [==============================] - 10s 582ms/step - loss: 0.6894 - accuracy: 0.5249 - val_loss: 0.6891 - val_accuracy: 0.5324\nEpoch 4/100\n18/18 [==============================] - 11s 646ms/step - loss: 0.6827 - accuracy: 0.5794 - val_loss: 0.6825 - val_accuracy: 0.5755\nEpoch 5/100\n18/18 [==============================] - 11s 618ms/step - loss: 0.6715 - accuracy: 0.6300 - val_loss: 0.6720 - val_accuracy: 0.6403\nEpoch 6/100\n18/18 [==============================] - 11s 602ms/step - loss: 0.6762 - accuracy: 0.5773 - val_loss: 0.6878 - val_accuracy: 0.5036\nEpoch 7/100\n18/18 [==============================] - 11s 612ms/step - loss: 0.6629 - accuracy: 0.5989 - val_loss: 0.6649 - val_accuracy: 0.6331\nEpoch 8/100\n18/18 [==============================] - 11s 635ms/step - loss: 0.6612 - accuracy: 0.6082 - val_loss: 0.6668 - val_accuracy: 0.5971\nEpoch 9/100\n18/18 [==============================] - 11s 628ms/step - loss: 0.6404 - accuracy: 0.6175 - val_loss: 0.6920 - val_accuracy: 0.5540\nEpoch 10/100\n18/18 [==============================] - 10s 575ms/step - loss: 0.6194 - accuracy: 0.6647 - val_loss: 0.6548 - val_accuracy: 0.6187\nEpoch 11/100\n18/18 [==============================] - 12s 653ms/step - loss: 0.6354 - accuracy: 0.6378 - val_loss: 0.6494 - val_accuracy: 0.6331\nEpoch 12/100\n18/18 [==============================] - 14s 770ms/step - loss: 0.6311 - accuracy: 0.6239 - val_loss: 0.6513 - val_accuracy: 0.6619\nEpoch 13/100\n18/18 [==============================] - 12s 675ms/step - loss: 0.6473 - accuracy: 0.5969 - val_loss: 0.6602 - val_accuracy: 0.6619\nEpoch 14/100\n18/18 [==============================] - 14s 805ms/step - loss: 0.6179 - accuracy: 0.6673 - val_loss: 0.6461 - val_accuracy: 0.6691\nEpoch 15/100\n18/18 [==============================] - 11s 649ms/step - loss: 0.6206 - accuracy: 0.6450 - val_loss: 0.6533 - val_accuracy: 0.6115\nEpoch 16/100\n18/18 [==============================] - 11s 610ms/step - loss: 0.6169 - accuracy: 0.6492 - val_loss: 0.6562 - val_accuracy: 0.5683\nEpoch 17/100\n18/18 [==============================] - 11s 608ms/step - loss: 0.6316 - accuracy: 0.6327 - val_loss: 0.6559 - val_accuracy: 0.6331\nEpoch 18/100\n18/18 [==============================] - 11s 611ms/step - loss: 0.5649 - accuracy: 0.6983 - val_loss: 0.6584 - val_accuracy: 0.6475\nEpoch 19/100\n18/18 [==============================] - 11s 628ms/step - loss: 0.5884 - accuracy: 0.6674 - val_loss: 0.6435 - val_accuracy: 0.6691\nEpoch 20/100\n18/18 [==============================] - 11s 611ms/step - loss: 0.6213 - accuracy: 0.6257 - val_loss: 0.6578 - val_accuracy: 0.6763\nEpoch 21/100\n18/18 [==============================] - 10s 570ms/step - loss: 0.6125 - accuracy: 0.6585 - val_loss: 0.6586 - val_accuracy: 0.6547\nEpoch 22/100\n18/18 [==============================] - 11s 625ms/step - loss: 0.6046 - accuracy: 0.6898 - val_loss: 0.6613 - val_accuracy: 0.6619\nEpoch 23/100\n18/18 [==============================] - 10s 576ms/step - loss: 0.6068 - accuracy: 0.6856 - val_loss: 0.6494 - val_accuracy: 0.6403\nEpoch 24/100\n18/18 [==============================] - 10s 579ms/step - loss: 0.5829 - accuracy: 0.6760 - val_loss: 0.6549 - val_accuracy: 0.6835\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Results","metadata":{}},{"cell_type":"code","source":"inputs = tf.keras.Input(shape=(224, 224, 3))\nx = tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu')(inputs)\nx = tf.keras.layers.MaxPool2D()(x)\nx = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(x)\nx = tf.keras.layers.MaxPool2D()(x)\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\nx = tf.keras.layers.Dense(128, activation='relu')(x)\nx = tf.keras.layers.Dense(128, activation='relu')(x)\noutputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel2 = tf.keras.Model(inputs=inputs, outputs=outputs)\n\nmodel2.compile(\n    optimizer='rmsprop',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory = model2.fit(\n    train_images,\n    validation_data=val_images,\n    epochs=100,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=5,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            patience=3\n        )\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T21:37:10.119110Z","iopub.execute_input":"2023-06-22T21:37:10.119489Z","iopub.status.idle":"2023-06-22T21:40:38.096166Z","shell.execute_reply.started":"2023-06-22T21:37:10.119453Z","shell.execute_reply":"2023-06-22T21:40:38.095308Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Epoch 1/100\n18/18 [==============================] - 12s 649ms/step - loss: 0.6931 - accuracy: 0.5286 - val_loss: 0.6920 - val_accuracy: 0.4820\nEpoch 2/100\n18/18 [==============================] - 10s 569ms/step - loss: 0.6918 - accuracy: 0.4865 - val_loss: 0.6896 - val_accuracy: 0.4964\nEpoch 3/100\n18/18 [==============================] - 10s 565ms/step - loss: 0.6869 - accuracy: 0.5266 - val_loss: 0.6898 - val_accuracy: 0.4964\nEpoch 4/100\n18/18 [==============================] - 11s 614ms/step - loss: 0.6869 - accuracy: 0.5502 - val_loss: 0.6751 - val_accuracy: 0.5827\nEpoch 5/100\n18/18 [==============================] - 10s 582ms/step - loss: 0.6721 - accuracy: 0.5801 - val_loss: 0.6790 - val_accuracy: 0.5755\nEpoch 6/100\n18/18 [==============================] - 10s 580ms/step - loss: 0.6754 - accuracy: 0.5938 - val_loss: 0.6598 - val_accuracy: 0.6043\nEpoch 7/100\n18/18 [==============================] - 11s 615ms/step - loss: 0.6631 - accuracy: 0.6012 - val_loss: 0.6739 - val_accuracy: 0.5899\nEpoch 8/100\n18/18 [==============================] - 10s 569ms/step - loss: 0.6607 - accuracy: 0.6203 - val_loss: 0.6532 - val_accuracy: 0.6619\nEpoch 9/100\n18/18 [==============================] - 10s 574ms/step - loss: 0.6432 - accuracy: 0.6273 - val_loss: 0.6592 - val_accuracy: 0.6619\nEpoch 10/100\n18/18 [==============================] - 11s 614ms/step - loss: 0.6454 - accuracy: 0.6191 - val_loss: 0.6512 - val_accuracy: 0.6259\nEpoch 11/100\n18/18 [==============================] - 11s 601ms/step - loss: 0.6381 - accuracy: 0.6136 - val_loss: 0.6487 - val_accuracy: 0.6619\nEpoch 12/100\n18/18 [==============================] - 11s 630ms/step - loss: 0.6402 - accuracy: 0.6380 - val_loss: 0.6518 - val_accuracy: 0.6475\nEpoch 13/100\n18/18 [==============================] - 12s 674ms/step - loss: 0.6259 - accuracy: 0.6612 - val_loss: 0.6610 - val_accuracy: 0.6115\nEpoch 14/100\n18/18 [==============================] - 11s 630ms/step - loss: 0.6530 - accuracy: 0.6238 - val_loss: 0.6443 - val_accuracy: 0.6619\nEpoch 15/100\n18/18 [==============================] - 11s 633ms/step - loss: 0.6513 - accuracy: 0.6262 - val_loss: 0.6822 - val_accuracy: 0.6043\nEpoch 16/100\n18/18 [==============================] - 12s 678ms/step - loss: 0.6359 - accuracy: 0.6282 - val_loss: 0.6474 - val_accuracy: 0.6547\nEpoch 17/100\n18/18 [==============================] - 11s 604ms/step - loss: 0.6428 - accuracy: 0.6404 - val_loss: 0.6529 - val_accuracy: 0.6403\nEpoch 18/100\n18/18 [==============================] - 11s 615ms/step - loss: 0.6299 - accuracy: 0.6148 - val_loss: 0.6474 - val_accuracy: 0.6331\nEpoch 19/100\n18/18 [==============================] - 10s 582ms/step - loss: 0.6140 - accuracy: 0.6859 - val_loss: 0.6544 - val_accuracy: 0.6403\n","output_type":"stream"}]},{"cell_type":"code","source":"results = model.evaluate(test_images, verbose=0)\n\nprint(\"    Test Loss: {:.5f}\".format(results[0]))\nprint(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))","metadata":{"execution":{"iopub.status.busy":"2023-06-22T21:41:04.132850Z","iopub.execute_input":"2023-06-22T21:41:04.133226Z","iopub.status.idle":"2023-06-22T21:41:08.311325Z","shell.execute_reply.started":"2023-06-22T21:41:04.133185Z","shell.execute_reply":"2023-06-22T21:41:08.310380Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"    Test Loss: 0.64906\nTest Accuracy: 66.00%\n","output_type":"stream"}]},{"cell_type":"code","source":"results2 = model2.evaluate(test_images, verbose=0)\n\nprint(\"    Test Loss: {:.5f}\".format(results2[0]))\nprint(\"Test Accuracy: {:.2f}%\".format(results2[1] * 100))","metadata":{"execution":{"iopub.status.busy":"2023-06-22T21:44:42.041194Z","iopub.execute_input":"2023-06-22T21:44:42.041623Z","iopub.status.idle":"2023-06-22T21:44:44.154441Z","shell.execute_reply.started":"2023-06-22T21:44:42.041579Z","shell.execute_reply":"2023-06-22T21:44:44.153243Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"    Test Loss: 0.64730\nTest Accuracy: 63.67%\n","output_type":"stream"}]},{"cell_type":"code","source":"predictions = (model.predict(test_images) >= 0.5).astype(np.int)\n\ncm = confusion_matrix(test_images.labels, predictions, labels=[0, 1])\nclr = classification_report(test_images.labels, predictions, labels=[0, 1], target_names=[\"CAT\", \"DOG\"])\n\nplt.figure(figsize=(6, 6))\nsns.heatmap(cm, annot=True, fmt='g', vmin=0, cmap='Blues', cbar=False)\nplt.xticks(ticks=[0.5, 1.5], labels=[\"CAT\", \"DOG\"])\nplt.yticks(ticks=[0.5, 1.5], labels=[\"CAT\", \"DOG\"])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\nprint(\"Classification Report:\\n----------------------\\n\", clr)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T21:41:15.049931Z","iopub.execute_input":"2023-06-22T21:41:15.050343Z","iopub.status.idle":"2023-06-22T21:41:17.463082Z","shell.execute_reply.started":"2023-06-22T21:41:15.050304Z","shell.execute_reply":"2023-06-22T21:41:17.462249Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x432 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"name":"stdout","text":"Classification Report:\n----------------------\n               precision    recall  f1-score   support\n\n         CAT       0.68      0.62      0.65       151\n         DOG       0.64      0.70      0.67       149\n\n    accuracy                           0.66       300\n   macro avg       0.66      0.66      0.66       300\nweighted avg       0.66      0.66      0.66       300\n\n","output_type":"stream"}]}]}